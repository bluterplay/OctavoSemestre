{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123a6256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 12:01:23.665985: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 12:01:24.030182: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-16 12:01:24.093858: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-16 12:01:24.093894: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-16 12:01:24.144916: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-16 12:01:25.535594: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-16 12:01:25.535769: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-16 12:01:25.535784: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b662d283",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 12:05:36.839158: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-16 12:05:36.839373: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-16 12:05:36.839427: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (efy): /proc/driver/nvidia/version does not exist\n",
      "2023-03-16 12:05:36.840034: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "clasificador = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72929f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador.add(Conv2D(filters=32,kernel_size=(3,3),input_shape=(64,64,3,),activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8af2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4ec0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6c6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador.add(Dense(units=128,activation=\"relu\"))\n",
    "# 128 nodos intermedios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b0f8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa de salida\n",
    "clasificador.add(Dense(units=1,activation='sigmoid'))\n",
    "# 1 --> Salida binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82569816",
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f98993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d51db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "\t\t\t\trescale=1./255,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.2,\n",
    "                horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f473a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51553483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_dataset = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                     target_size=(64,64),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f597a7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                                     target_size=(64,64),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95404518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12346/2177862880.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  clasificador.fit_generator(training_dataset,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 23s 94ms/step - loss: 0.5340 - accuracy: 0.7325\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 0.5169 - accuracy: 0.7374\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.5117 - accuracy: 0.7426\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4976 - accuracy: 0.7509\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4908 - accuracy: 0.7648\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4766 - accuracy: 0.7691\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 23s 91ms/step - loss: 0.4700 - accuracy: 0.7689\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4649 - accuracy: 0.7790\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4586 - accuracy: 0.7796\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4485 - accuracy: 0.7854\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4433 - accuracy: 0.7922\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4360 - accuracy: 0.7936\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.4314 - accuracy: 0.7949\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4210 - accuracy: 0.8011\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.4226 - accuracy: 0.8037\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4124 - accuracy: 0.8079\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4113 - accuracy: 0.8095\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4039 - accuracy: 0.8164\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.3958 - accuracy: 0.8210\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.3958 - accuracy: 0.8156\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.3836 - accuracy: 0.8231\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.3793 - accuracy: 0.8294\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.3695 - accuracy: 0.8322\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.3757 - accuracy: 0.8292\n",
      "Epoch 25/25\n",
      "180/250 [====================>.........] - ETA: 6s - loss: 0.3582 - accuracy: 0.8389"
     ]
    }
   ],
   "source": [
    "clasificador.fit_generator(training_dataset,\n",
    "                          steps_per_epoch=20, \n",
    "                          epochs=25,\n",
    "                          validation_data = testing_dataset,\n",
    "                          validation_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediccion\n",
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = image.load_img('dataset/single/omagen.jpg',target_size=(64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "result = classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0]==1:\n",
    "    prediction='dog'\n",
    "else:\n",
    "    prediction ='cat'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
